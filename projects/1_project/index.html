<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>MiRo Bio-inspired Visual-System | Zheng Li</title> <meta name="author" content="Zheng Li"> <meta name="description" content="Apply Computer Vision algorihtms &amp; a bio-inspired theory on a biomimetic robot named MiRo."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lizheng1997.github.io/projects/1_project/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Zheng </span>Li</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">MiRo Bio-inspired Visual-System</h1> <p class="post-description">Apply Computer Vision algorihtms &amp; a bio-inspired theory on a biomimetic robot named MiRo.</p> </header> <article> <p>This is my first version of an bio-inspired visual system which is based on OpenCV APIs and ANN, and the final aim of this project is intercepting a moving target. I pre-defined three targets, named pedestrians, MiRo robots and MiRo toy balls. In total, there are three scenarios for different deploying situations, like on-board; off-board and in a simulator (Gazebo). Some codes are inspired from code samples in the MDK-2019 version. Thanks for their brilliant work on building MiRo robots. I am also one of the big fan loving biomimetic robots. Link: http://labs.consequentialrobotics.com/miro-e/docs/index.php?page=Introduction</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/MiRo-tracking-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/MiRo-tracking-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/MiRo-tracking-1400.webp"></source> <img src="/assets/img/MiRo-tracking.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="miro-tracking-miro" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/MiRo-tracking-pedestrian-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/MiRo-tracking-pedestrian-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/MiRo-tracking-pedestrian-1400.webp"></source> <img src="/assets/img/MiRo-tracking-pedestrian.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="miro-tracking-people" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: A MiRo robot is tracking and following another MiRo robot. Right: A MiRo robot wants to intercept and catch a pedestrian. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/demo_intercepting_ball.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/demo_intercepting_miro_without_safety_control_14.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> </div> </div> <div class="caption"> Left: A MiRo robot is intercepting a ball Right: A MiRo robot is intercepting a miro robot without safety control. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/demo_real_world_miro.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/demo_real_world_pedestrian.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> </div> </div> <div class="caption"> Left: A MiRo robot is tracking and following another MiRo robot. Right: A MiRo robot wants to intercept and catch a pedestrian. </div> <p><strong><em>Branches Notes:</em></strong></p> <p>The master branch is the official released version, and the core version is the core part of this system, also the dev branch contains all developed codes.</p> <ul> <li><a href="#news">News</a></li> <li> <a href="#requirements">Requirements</a> <ul> <li><a href="#software">Software</a></li> <li><a href="#hardware">Hardware</a></li> <li><a href="#models">Models</a></li> <li><a href="#start-miro_cv-node">Start miro_cv Node</a></li> <li><a href="#1-background">1. Background</a></li> <li> <a href="#2-methods">2. Methods</a> <ul> <li><a href="#21-braitenberg-vehicles">2.1 Braitenberg vehicles</a></li> <li><a href="#22-the-kinematics-of-interception-task">2.2 The Kinematics of interception task</a></li> </ul> </li> <li> <a href="#3-framework">3. Framework</a> <ul> <li> <a href="#31-detector-module">3.1 Detector Module</a> <ul> <li><a href="#311-miro-detection-node">3.1.1. MiRo Detection Node</a></li> <li><a href="#312-ball-detection-node">3.1.2. Ball Detection Node</a></li> <li><a href="#313-pedestrian-detection-node">3.1.3. Pedestrian Detection Node</a></li> </ul> </li> <li> <a href="#32-tracker-module">3.2 Tracker Module</a> <ul> <li><a href="#321-single-tracking-node">3.2.1. Single-Tracking Node</a></li> <li><a href="#322-multi-tracking-node">3.2.2. Multi-Tracking Node</a></li> </ul> </li> <li> <a href="#33-control-module">3.3 Control Module</a> <ul> <li><a href="#331-orientation-control-node">3.3.1. Orientation Control Node</a></li> <li><a href="#332-safety-controller-node">3.3.2. Safety Controller Node</a></li> </ul> </li> <li> <a href="#34-path-planning-module">3.4. Path Planning Module</a> <ul> <li><a href="#341-path-planning-node">3.4.1. Path planning Node</a></li> </ul> </li> <li> <a href="#35-localization-module">3.5 Localization Module</a> <ul> <li><a href="#351-kalman-filter-pose-estimation-node">3.5.1. Kalman Filter Pose Estimation Node</a></li> <li><a href="#352-wheel-odometry-node">3.5.2. Wheel Odometry Node</a></li> <li><a href="#353-visual-odometry-node">3.5.3. Visual Odometry Node</a></li> </ul> </li> </ul> </li> <li><a href="#4-summary">4. Summary</a></li> <li><a href="#more-about-miro-robot">More about MiRo Robot</a></li> <li><a href="#todo">TODO</a></li> <li><a href="#references">References</a></li> </ul> </li> </ul> <h1 id="news">News</h1> <p><strong>2023-10</strong> MDK-20230105 is supported now!</p> <p><strong>2023-09</strong> ROS Noetic is supported now!</p> <h1 id="requirements">Requirements</h1> <h2 id="software">Software</h2> <ol> <li>Ubuntu20.04</li> <li>ROS Noetic</li> <li>MiRo MDK 20230105</li> <li>OpenCV &gt; 3.4</li> <li>Python</li> </ol> <h2 id="hardware">Hardware</h2> <ol> <li>a MiRo robot</li> <li>a Laptop with GPU card is better</li> </ol> <h2 id="models">Models</h2> <ol> <li>miro_cascases: models/miro_cascades</li> <li>pedestrian_bin: models/pedestrian_bin</li> </ol> <h2 id="start-miro_cv-node">Start miro_cv Node</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cd your_workspace &amp;&amp; catkin_make
$ source devel/setup.bash
$ roslaunch miro_cv miro_cv.launch
</code></pre></div></div> <h2 id="1-background">1. Background</h2> <p>First of all, this project is my Master’s dissertation project in 2019, now I uploaded all codes and more improvements will be implemented in the future. All relevant resources will be uploaded, including training sample images and model files. You can find some model files in the root path of this repo, a fold named <strong><em>models</em></strong>. Training sample images can be found here: <a href="https://drive.google.com/drive/folders/1owF3loF_p_iO_xP7X3yBjYI6e0c9NLm3?usp=sharing" rel="external nofollow noopener" target="_blank">Google Drive</a></p> <h2 id="2-methods">2. Methods</h2> <p>This CV system is mainly based on <strong>OpenCV library</strong>, which uses some conventional computer vision algorithms, like <strong>Hough circle detection</strong>, <strong>Gaussian filter</strong>, <strong>Median filter</strong>, <strong>Optical flow estimation</strong> and <strong>stereo camera depth calculation</strong>, etc. Also, <strong>Deep Neural Network models</strong> are used in this project to achieve real time detection. Apart from perception algorithms, a <strong>bio-inspired method</strong> is also designed in this system using a end-to-end paradigm, it refers to the famous <strong><a href="https://en.wikipedia.org/wiki/Braitenberg_vehicle" rel="external nofollow noopener" target="_blank">Braitenberg vehicle</a> theory</strong>. Path planning module and Localization module are not added into this system yet now.</p> <h3 id="21-braitenberg-vehicles">2.1 Braitenberg vehicles</h3> <p>In this system, I implemented two kinds of braitenberg vehicles, <strong>2b (aggression) vehicle</strong> and <strong>3a (love) vehicle</strong>, you can find these two types in following two pictures.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/image-20240805133159394-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/image-20240805133159394-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/image-20240805133159394-1400.webp"></source> <img src="/assets/img/image-20240805133159394.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="2b" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/image-20240804165358095-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/image-20240804165358095-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/image-20240804165358095-1400.webp"></source> <img src="/assets/img/image-20240804165358095.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="3a" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="22-the-kinematics-of-interception-task">2.2 The Kinematics of interception task</h3> <p>The first picture shows the kinematic model about that a miro robot wants to intercept another miro robot.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/image-20240804171420804-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/image-20240804171420804-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/image-20240804171420804-1400.webp"></source> <img src="/assets/img/image-20240804171420804.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="kinematic model" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/image-20240804172129418-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/image-20240804172129418-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/image-20240804172129418-1400.webp"></source> <img src="/assets/img/image-20240804172129418.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="kinematic model" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="3-framework">3. Framework</h2> <h3 id="31-detector-module">3.1 Detector Module</h3> <h4 id="311-miro-detection-node">3.1.1. MiRo Detection Node</h4> <p>This miro robot detection node is based on Cascade Classifier, the classifier model is trained using collected indoor MiRo robots’ appearance data. A positive sample is a MiRo robot in the center of an image , a negative sample is some other indoor objects in a lab. Hence, this cascade classifier needs to predict whether there is a MiRo robot in one frame.</p> <h4 id="312-ball-detection-node">3.1.2. Ball Detection Node</h4> <p>This node is based on the <strong>Hough Circle</strong> detection in the OpenCV lib. And this node might better use in a Gazebo simulator for showing a better effect. The reason is that the real world environment is sometimes too noisy for this detection algorithm to work well, the effect of detection is unstable.</p> <h4 id="313-pedestrian-detection-node">3.1.3. Pedestrian Detection Node</h4> <p>This node is based on a DNN module in the OpenCV lib or independent models. Using some traditional CNN model, like darknet, faster-rcnn and yolov3, to achieve the functionality of pedestrian detection in real world with a considerable latency.</p> <h3 id="32-tracker-module">3.2 Tracker Module</h3> <h4 id="321-single-tracking-node">3.2.1. Single-Tracking Node</h4> <p>This node is based on the tracker APIs in OpenCV . There are 8 trackers in total after opencv3.4 version. So, if you want to use this project, you have to use a higher or equal version than opecnv3.4, or some unexpected errors might encounter. The TLD tracker is the one I prefer to use in indoor environment with occlusion situation.</p> <h4 id="322-multi-tracking-node">3.2.2. Multi-Tracking Node</h4> <p>This node is based on the multitracker API in OpenCV.</p> <h3 id="33-control-module">3.3 Control Module</h3> <p>The Control module contains a direction keeper node, this is the main function for maintaining the yaw angle of a MiRo robot.</p> <h4 id="331-orientation-control-node">3.3.1. Orientation Control Node</h4> <p>This node is based on a designed <strong>bio-inspired</strong> orientation correction algorithms. Orientation errors are calculated based on the yaw angle variations between the view line and the ego-to-target line. Hence, the MiRo robot will be driven to point its head towards the target and keep following the target, then it can intercept this moving target with considerable higher linear velocity. Moreover, the angular velocity will be produced based on the angle variation mentioned before.</p> <h4 id="332-safety-controller-node">3.3.2. Safety Controller Node</h4> <p>This node is based on a sonar sensor on the MiRo robot’s nose. I use the raw data from the sonar sensor to estimate the distance between the target and the MiRo robot. This is not accurate enough in real time tests.</p> <h3 id="34-path-planning-module">3.4. Path Planning Module</h3> <h4 id="341-path-planning-node">3.4.1. Path planning Node</h4> <p>Global path planning and Local path planning are designed for generating paths of intercepting another moving target. A kinematic model is necessary for this node to work smoothly in real time. But, in this version, I did not add path planning module to produce real time planning paths. Instead, I merely use aforementioned Orientation Control module to implement a bio-inspired way of approaching a moving target without applying a planning method.</p> <h3 id="35-localization-module">3.5 Localization Module</h3> <h4 id="351-kalman-filter-pose-estimation-node">3.5.1. Kalman Filter Pose Estimation Node</h4> <p>Using last second pose and utilize Kalman Filter to predict the current ego pose. Still, this node is not added to the first version, because the objective of using a pose estimation node is utilized in the path planning module.</p> <h4 id="352-wheel-odometry-node">3.5.2. Wheel Odometry Node</h4> <p>This wheel odometry node is mainly designed for localizing the MiRo robot’s global position, but accumulated errors should be considered and drift issue might need be solved.</p> <h4 id="353-visual-odometry-node">3.5.3. Visual Odometry Node</h4> <p>This visual odometry node is used for improving the localization accuracy, and it will be fused with wheel odometry to generate a more accurate global position.</p> <h2 id="4-summary">4. Summary</h2> <p>All codes are implemented based on doing self-learning roughly in 5 months. Necessary knowledge includes ROS, Computer Vision basic theories, OpenCV APIs, Physical Robots’ experiments, Linux system, Machine Learning, Math analysis on kinematic models and so on.</p> <h2 id="more-about-miro-robot">More about MiRo Robot</h2> <p>If you want to know more about this biomimetic MiRo robot, please click this <a href="https://www.miro-e.com/robot" rel="external nofollow noopener" target="_blank">link</a> to find more on their official websites.</p> <h2 id="todo">TODO</h2> <ul class="task-list"> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled checked>Simplify codes to a core version</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled checked>Update the compatible ROS version to ROS1 Noetic version</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled checked>Update the MDK version to MDK202301</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Add and test the Wheel Odometry node</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Add and test the Visual Odometry node</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>Add the path planning module into the system for comparation</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled>update previous bio-inspired and RL methods</li> </ul> <h2 id="references">References</h2> <p>[1] Belkhouche, F. and Belkhouche, B., 2004, December. On the tracking and interception of a moving object by a wheeled mobile robot. In <em>IEEE Conference on Robotics, Automation and Mechatronics, 2004.</em> (Vol. 1, pp. 130-135). IEEE.</p> <p>[2] Headleand, C.J. and Teahan, W., 2016, July. Towards ethical robots: Revisiting Braitenberg’s vehicles. In <em>2016 SAI Computing Conference (SAI)</em> (pp. 469-477). IEEE.</p> <p>[3] Application of Computer Vision for A Biomimetic Robot</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Zheng Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>